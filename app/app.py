# app/app.py

import streamlit as st
import json
import os
from dotenv import load_dotenv # dotenv ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# src ë° visualization ëª¨ë“ˆ ì„í¬íŠ¸
from src.scraper import fetch_hashtag_posts, save_posts_to_json
from src.cleaner import clean_captions
from src.sentiment import load_sentiment_model, analyze_sentiment
from src.keywords import load_keyword_model, extract_keywords, get_keyword_frequency # get_keyword_frequencyëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ê¸°ì¡´ ì½”ë“œì— ìˆì—ˆìœ¼ë¯€ë¡œ ìœ ì§€
from src.search import search_by_keyword
from visualization.charts import plot_sentiment_distribution
from visualization.wordcloud import generate_wordcloud

# Langchain ë° Upstage ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
API_KEY = os.getenv("UPSTAGE_API_KEY") # Upstage API í‚¤ ë¡œë“œ

# ì „ì—­ ì„¤ì • ë³€ìˆ˜
DATA_DIR = "../data"
FONT_PATH = "c:/Windows/Fonts/malgun.ttf" # í•œê¸€ ì§€ì› í°íŠ¸ (ìœˆë„ìš° ê¸°ì¤€)


def build_vectorstore_from_posts(posts: list) -> FAISS:
    """
    ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ì—ì„œ cleaned_captionì„ ì‚¬ìš©í•˜ì—¬ FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    """
    # cleaned_captionì´ ìˆëŠ” ê²Œì‹œê¸€ë§Œ Documentë¡œ ë³€í™˜
    docs = [Document(page_content=p["cleaned_caption"]) for p in posts if "cleaned_caption" in p]
    embeddings = UpstageEmbeddings()
    return FAISS.from_documents(docs, embeddings)


def solar_rag_answer_multi(question: str, history: list, vectorstore: FAISS, k: int = 5,
                           model_name: str = "solar-1-mini-chat", api_key: str = None) -> str:
    """
    Solar APIì™€ RAG, ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    :param question: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
    :param history: ì´ì „ ëŒ€í™” ì´ë ¥ (ì§ˆë¬¸, ë‹µë³€ íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
    :param vectorstore: êµ¬ì¶•ëœ FAISS ë²¡í„° ìŠ¤í† ì–´
    :param k: ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œì˜ ìˆ˜
    :param model_name: ì‚¬ìš©í•  Solar ëª¨ë¸ ì´ë¦„
    :param api_key: Upstage API í‚¤
    :return: Solar ëª¨ë¸ì˜ ë‹µë³€
    """
    if not api_key:
        return "ì˜¤ë¥˜: Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

    # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    context_docs = retriever.invoke(question)
    context = "\n".join([doc.page_content for doc in context_docs])

    # ì´ì „ ëŒ€í™” ì´ë ¥ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ê¸° ìœ„í•´ í¬ë§·íŒ…
    chat_history = "\n".join([f"ì‚¬ìš©ì: {q}\nAI: {a}" for q, a in history])

    # ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì€ ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ìˆ˜ì§‘í•œ ê²Œì‹œê¸€ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì™€ ê²Œì‹œê¸€ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ê²Œì‹œê¸€ ë‚´ìš©ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ê±°ë‚˜, ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì—ëŠ” "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê²Œì‹œê¸€ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{chat_history}

[ê²Œì‹œê¸€ ë‚´ìš©]
{context}

[í˜„ì¬ ì§ˆë¬¸]
{question}

[AIì˜ ë‹µë³€]
""")
    
    # Solar ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatUpstage(model=model_name, api_key=api_key)
    
    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±
    chain = prompt | llm
    
    # ì²´ì¸ ì‹¤í–‰ ë° ì‘ë‹µ ë°˜í™˜
    response = chain.invoke({
        "question": question,
        "context": context,
        "chat_history": chat_history
    })
    return response.content


def run_pipeline(hashtag, max_posts):
    """
    ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¶€í„° í‚¤ì›Œë“œ ì¶”ì¶œê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    st.info("ğŸ“¥ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘...")
    try:
        posts = fetch_hashtag_posts(hashtag, max_count=max_posts)
        save_path = os.path.join(DATA_DIR, f"{hashtag}_raw.json")
        save_posts_to_json(posts, save_path)
        st.success(f"âœ… ê²Œì‹œê¸€ {len(posts)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    except Exception as e:
        st.error(f"âŒ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.warning("InstaloaderëŠ” ë¹„ê³µì‹ APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, IP ì°¨ë‹¨ì´ë‚˜ ìº¡ì±  ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, ë”œë ˆì´ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    if not posts: # ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìœ¼ë©´ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
        st.warning("ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. í•´ì‹œíƒœê·¸ë‚˜ ìˆ˜ì§‘ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

    st.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
    posts = clean_captions(posts)

    st.info("ğŸ“Š ê°ì • ë¶„ì„ ì¤‘...")
    sentiment_model = load_sentiment_model()
    posts = analyze_sentiment(posts, sentiment_model)

    st.info("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    keyword_model = load_keyword_model()
    posts = extract_keywords(posts, keyword_model)

    output_path = os.path.join(DATA_DIR, f"{hashtag}_final.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(posts, f, ensure_ascii=False, indent=2)

    st.success("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    return posts


def main():
    st.set_page_config(page_title="ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸°", layout="wide")
    st.title("ğŸ“¸ ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸° (ê³µê°œ ë°ì´í„° ê¸°ë°˜)")

    # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
    if not API_KEY:
        st.error("âŒ UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜, API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop() # API í‚¤ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨

    # íƒ­ ë©”ë‰´ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ” í•´ì‹œíƒœê·¸ ë¶„ì„", "ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰", "ğŸ§  ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ (ë©€í‹°í„´)"])

    with tab1:
        st.header("í•´ì‹œíƒœê·¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸")
        hashtag = st.text_input("ë¶„ì„í•  í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì—¬í–‰, ootd):").strip().replace("#", "")
        max_posts = st.slider("ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜", min_value=20, max_value=500, value=100, step=10)

        if st.button("ë¶„ì„ ì‹œì‘"):
            if hashtag:
                # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥
                posts = run_pipeline(hashtag, max_posts)
                
                if posts: # ê²Œì‹œê¸€ì´ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ ë° ë¶„ì„ë˜ì—ˆì„ ê²½ìš°
                    st.session_state["analyzed_posts"] = posts # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    
                    # ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë° ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (RAG íƒ­ì—ì„œ ì¬ì‚¬ìš©)
                    with st.spinner("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘..."):
                        st.session_state["vectorstore"] = build_vectorstore_from_posts(posts)
                    st.success("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")

                    st.subheader("ğŸ“ˆ ê°ì • ë¶„ì„ ê²°ê³¼")
                    plot_sentiment_distribution(posts)

                    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    generate_wordcloud(posts, font_path=FONT_PATH)
                else:
                    st.warning("ë¶„ì„ì„ ìœ„í•œ ê²Œì‹œê¸€ì´ ì¶©ë¶„íˆ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            else:
                st.warning("í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    with tab2:
        st.header("í‚¤ì›Œë“œ ê²€ìƒ‰")
        # 'analyzed_posts'ê°€ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ”ì§€ í™•ì¸
        posts = st.session_state.get("analyzed_posts", [])

        if posts:
            st.info("ì¢Œì¸¡ 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            search_term = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥:")
            if search_term:
                results = search_by_keyword(posts, search_term)
                st.write(f"'{search_term}' í‚¤ì›Œë“œ í¬í•¨ ê²Œì‹œê¸€: {len(results)}ê°œ")
                # ê²€ìƒ‰ ê²°ê³¼ëŠ” 5ê°œê¹Œì§€ë§Œ í‘œì‹œ
                for i, post in enumerate(results[:5], 1):
                    st.markdown(f"**{i}.** {post['cleaned_caption']}")
            elif search_term == "": # ì…ë ¥ì°½ì´ ë¹„ì–´ìˆì„ ë•Œ
                st.info("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.info("ë¨¼ì € 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•´ ì£¼ì„¸ìš”.")

    with tab3:
        st.header("ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ (ë©€í‹°í„´)")
        # 'analyzed_posts'ì™€ 'vectorstore'ê°€ ì„¸ì…˜ ìƒíƒœì— ìˆëŠ”ì§€ í™•ì¸
        posts = st.session_state.get("analyzed_posts", [])
        vectorstore = st.session_state.get("vectorstore", None)

        if posts and vectorstore:
            st.info("ì¢Œì¸¡ 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
            
            # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
            if "chat_history" not in st.session_state:
                st.session_state.chat_history = []

            # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
            question = st.text_input("ğŸ—£ ê²Œì‹œê¸€ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”:", key="multi_q")

            # ì§ˆë¬¸ ë³´ë‚´ê¸° ë²„íŠ¼
            if st.button("ğŸ’¬ ì§ˆë¬¸ ë³´ë‚´ê¸°"):
                if question:
                    with st.spinner("Solar ì‘ë‹µ ìƒì„± ì¤‘..."):
                        answer = solar_rag_answer_multi(
                            question=question,
                            history=st.session_state.chat_history,
                            vectorstore=vectorstore, # êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ ì „ë‹¬
                            api_key=API_KEY
                        )
                        st.session_state.chat_history.append((question, answer))
                else:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™” ë²„íŠ¼
            if st.button("ğŸ§¹ ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”"):
                st.session_state.chat_history = []
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

            st.markdown("---")
            st.markdown("## ğŸ’¬ ëŒ€í™” ì´ë ¥")
            # ìµœì‹  ëŒ€í™”ê°€ ìœ„ì— ì˜¤ë„ë¡ ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
            if st.session_state.chat_history:
                for idx, (q, a) in enumerate(st.session_state.chat_history[::-1], 1):
                    with st.expander(f"Q{len(st.session_state.chat_history) - idx + 1}: {q}"): # ì§ˆë¬¸ ë²ˆí˜¸ ì¡°ì •
                        st.markdown(f"**AI:** {a}")
            else:
                st.info("ì•„ì§ ëŒ€í™” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì‹œì‘í•´ ë³´ì„¸ìš”!")
        else:
            st.info("ë¨¼ì € 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•´ ì£¼ì„¸ìš”. (ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• í•„ìš”)")


if __name__ == "__main__":
    main()
