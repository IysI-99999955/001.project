# app/app.py

import sys
import os
from datetime import datetime, timedelta # datetime, timedelta ì„í¬íŠ¸ ì¶”ê°€

# --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ì‹œì‘ ---
current_script_path = os.path.abspath(__file__)
current_dir = os.path.dirname(current_script_path)
project_root_dir = os.path.join(current_dir, "..")
sys.path.insert(0, project_root_dir)
# --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ë ---

from collections import Counter
from typing import List, Dict

import streamlit as st
from dotenv import load_dotenv

# src ë° visualization ëª¨ë“ˆ ì„í¬íŠ¸
from src.cleaner import clean_captions
from src.sentiment import load_sentiment_model, analyze_sentiment
from src.keywords import load_keyword_model, extract_keywords, get_keyword_frequency
from src.search import search_by_keyword
from visualization.charts import plot_sentiment_distribution
from visualization.wordcloud import generate_wordcloud

# Langchain ë° Upstage ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate

# Apify í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from apify_client import ApifyClient

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")

# ì „ì—­ ì„¤ì • ë³€ìˆ˜
DATA_DIR = "../data"
FONT_PATH = "c:/Windows/Fonts/malgun.ttf" # í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”

# Apify Instagram Hashtag Scraper Actor ID
# ì´ IDë¥¼ ì‚¬ìš©í•˜ë ¤ëŠ” Instagram Hashtag Scraper Actorì˜ ì •í™•í•œ IDë¡œ êµì²´í•´ì£¼ì„¸ìš”.
# ì˜ˆ: "apify/instagram-hashtag-scraper" ë˜ëŠ” "novi/tiktok-hashtag-api" (í‹±í†¡ìš©ì´ì§€ë§Œ, ì¸ìŠ¤íƒ€ í•´ì‹œíƒœê·¸ ìŠ¤í¬ë˜í¼ë„ ìœ ì‚¬í•œ ì´ë¦„ì¼ ìˆ˜ ìˆìŒ)
INSTAGRAM_SCRAPER_ACTOR_ID = "apify/instagram-hashtag-scraper" 


def build_vectorstore_from_posts(posts: list) -> FAISS:
    """
    ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ì—ì„œ 'cleaned_caption'ì„ ì‚¬ìš©í•˜ì—¬ FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    """
    docs = [Document(page_content=p["cleaned_caption"]) for p in posts if "cleaned_caption" in p]
    # UpstageEmbeddings ì´ˆê¸°í™” ì‹œ 'model' íŒŒë¼ë¯¸í„° ì¶”ê°€
    embeddings = UpstageEmbeddings(model="embedding-query") # ë˜ëŠ” "embedding-passage" ë“±
    return FAISS.from_documents(docs, embeddings)


def solar_rag_answer_multi(question: str, history: list, vectorstore: FAISS, k: int = 5,
                           model_name: str = "solar-1-mini-chat", api_key: str = None) -> str:
    """
    Solar APIì™€ RAG, ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    """
    if not api_key:
        return "ì˜¤ë¥˜: Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    context_docs = retriever.invoke(question)
    context = "\n".join([doc.page_content for doc in context_docs])

    chat_history = "\n".join([f"ì‚¬ìš©ì: {q}\nAI: {a}" for q, a in history])

    prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì€ ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ìˆ˜ì§‘í•œ ê²Œì‹œê¸€ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì™€ ê²Œì‹œê¸€ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì‹¤í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ê²Œì‹œê¸€ ë‚´ìš©ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ê±°ë‚˜, ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì—ëŠ” "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê²Œì‹œê¸€ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{chat_history}

[ê²Œì‹œê¸€ ë‚´ìš©]
{context}

[í˜„ì¬ ì§ˆë¬¸]
{question}

[AIì˜ ë‹µë³€]
""")
    
    llm = ChatUpstage(model=model_name, api_key=api_key)
    chain = prompt | llm
    
    response = chain.invoke({
        "question": question,
        "context": context,
        "chat_history": chat_history
    })
    return response.content


def fetch_posts_from_apify(
    hashtag: str,
    max_count: int,
    apify_api_token: str
) -> List[Dict]:
    """
    Apify Instagram Hashtag Scraper Actorë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ì‹œíƒœê·¸ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    if not apify_api_token:
        st.error("âŒ Apify API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

    client = ApifyClient(apify_api_token)
    actor_id = INSTAGRAM_SCRAPER_ACTOR_ID # Instagram Hashtag Scraper Actor ID ì‚¬ìš©

    run_input = {
        "hashtags": [hashtag],
        "resultsLimit": max_count,
        # Instagram Hashtag ScraperëŠ” 'resultsType' íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë§Œì•½ ì‚¬ìš©í•˜ì‹œëŠ” íŠ¹ì • Actorê°€ 'posts' ë˜ëŠ” 'reels' êµ¬ë¶„ì„ ì§€ì›í•œë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.
        # "resultsType": "posts", 
        "proxyConfiguration": { "use": "AUTO_POOL" },
        "extendOutputFunction": """
            async ({ data, item, page, request, customData, basicCrawler, Apify }) => {
                return item;
            }
        """,
        "extendOutputFunctionVars": {},
    }

    st.info(f"ğŸš€ Apify ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì¤‘... í•´ì‹œíƒœê·¸: #{hashtag}, ìµœëŒ€ {max_count}ê°œ ê²Œì‹œê¸€")
    st.info(f"Apify ì½˜ì†”ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://console.apify.com/actors/{actor_id}")

    try:
        run = client.actor(actor_id).call(
            run_input=run_input,
            timeout_secs=300 # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        apify_posts = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            # Apify Actorì˜ ì¶œë ¥ ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ í•„ë“œëª…ì„ ì¡°ì •í•©ë‹ˆë‹¤.
            # ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í¬ë˜í¼ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 'caption', 'timestamp', 'likesCount', 'commentsCount', 'shortcode', 'url'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            caption = item.get("caption", "")
            
            date_str = item.get("timestamp")
            if date_str:
                # ISO 8601 í˜•ì‹ (ì˜ˆ: 2023-10-26T10:00:00.000Z)ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ
                date_only = date_str.split("T")[0] if "T" in date_str else date_str.split(" ")[0]
            else:
                date_only = None

            if caption:
                apify_posts.append({
                    "caption": caption,
                    "date": date_only,
                    "shortcode": item.get("shortcode"),
                    "url": item.get("url"),
                    "hashtag": hashtag, # ì…ë ¥ í•´ì‹œíƒœê·¸ ì €ì¥
                    "likes": item.get("likesCount", 0),
                    "comments": item.get("commentsCount", 0),
                    "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                })
        st.success(f"âœ… Apifyì—ì„œ {len(apify_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ.")
        return apify_posts

    except Exception as e:
        st.error(f"âŒ Apify ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.warning("Apify API í† í°, Actor ID, ë˜ëŠ” í¬ë ˆë”§ ì”ì•¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”. ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")
        return []

def _save_posts_to_json(posts: List[Dict], save_path: str) -> None:
    """JSON íŒŒì¼ ì €ì¥ (ì„ì‹œ í•¨ìˆ˜, src/utils.py ë“±ìœ¼ë¡œ ë¶„ë¦¬ ê¶Œì¥)"""
    import json # <-- ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ json ëª¨ë“ˆì„ ëª…ì‹œì ìœ¼ë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    try:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def filter_recent_posts(posts: List[Dict], days: int = 1) -> List[Dict]:
    """
    ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœê·¼ Nì¼ ì´ë‚´ì˜ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    :param posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ (ê° ê²Œì‹œë¬¼ì— 'date' í•„ë“œ í•„ìš”, YYYY-MM-DD í˜•ì‹)
    :param days: ìµœê·¼ Nì¼ (ê¸°ë³¸ê°’ 1ì¼)
    :return: í•„í„°ë§ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    recent_posts = []
    time_threshold = datetime.now() - timedelta(days=days)
    for post in posts:
        post_date_str = post.get("date")
        if post_date_str:
            try:
                # ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
                post_date = datetime.strptime(post_date_str, "%Y-%m-%d")
                if post_date >= time_threshold:
                    recent_posts.append(post)
            except ValueError:
                # ë‚ ì§œ í˜•ì‹ì´ ë‹¤ë¥´ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.
                continue
    return recent_posts


def run_pipeline(hashtag: str, max_posts: int):
    """
    ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¶€í„° í‚¤ì›Œë“œ ì¶”ì¶œê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    progress_text = st.empty()
    progress_bar = st.progress(0)

    progress_text.text(f"ğŸ“¥ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘... (Apify Actor ì‹¤í–‰)")
    
    try:
        posts = fetch_posts_from_apify(hashtag, max_posts, APIFY_API_TOKEN)
        
        # ìµœê·¼ 1ì¼ ì´ë‚´ ê²Œì‹œë¬¼ í•„í„°ë§
        initial_collected_count = len(posts)
        posts = filter_recent_posts(posts, days=1) 
        if initial_collected_count > 0: # ì´ˆê¸° ìˆ˜ì§‘ ê²Œì‹œë¬¼ì´ ìˆì„ ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ í‘œì‹œ
            st.info(f"â³ ìµœê·¼ 1ì¼ ì´ë‚´ ê²Œì‹œê¸€ {len(posts)}ê°œ í•„í„°ë§ ì™„ë£Œ (ì´ {initial_collected_count}ê°œ ì¤‘).")

        save_path = os.path.join(DATA_DIR, f"instagram_{hashtag}_raw.json")
        _save_posts_to_json(posts, save_path) # <-- ì—¬ê¸°ì„œ json.dumpë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

        progress_bar.progress(100)
        progress_text.text(f"âœ… ê²Œì‹œê¸€ {len(posts)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        st.success(f"âœ… ê²Œì‹œê¸€ {len(posts)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        st.error(f"âŒ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.warning("Apify ìŠ¤í¬ë˜í•‘ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Apify ì½˜ì†”ì—ì„œ Actor ì‹¤í–‰ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        progress_text.empty()
        progress_bar.empty()
        return []

    if not posts:
        st.warning("ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. í•´ì‹œíƒœê·¸ë‚˜ ìˆ˜ì§‘ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

    st.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
    posts = clean_captions(posts)

    st.info("ğŸ“Š ê°ì • ë¶„ì„ ì¤‘...")
    sentiment_model = load_sentiment_model()
    posts = analyze_sentiment(posts, sentiment_model)

    st.info("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    keyword_model = load_keyword_model()
    posts = extract_keywords(posts, keyword_model)

    output_path = os.path.join(DATA_DIR, f"instagram_{hashtag}_final.json")
    with open(output_path, "w", encoding="utf-8") as f:
        # json.dump(posts, f, ensure_ascii=False, indent=2) # <-- ì´ ì¤„ì€ ì´ë¯¸ ì™¸ë¶€ì— import jsonì´ ìˆìœ¼ë¯€ë¡œ ì œê±°í•´ë„ ë©ë‹ˆë‹¤.
        import json # <-- ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ json ëª¨ë“ˆì„ ëª…ì‹œì ìœ¼ë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
        json.dump(posts, f, ensure_ascii=False, indent=2)

    st.success("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    return posts

def get_hashtag_frequency(posts: List[Dict], top_n: int = 10) -> List[tuple]:
    """
    ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ë‚´ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í•´ì‹œíƒœê·¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ì£¼ì˜: í˜„ì¬ëŠ” ìº¡ì…˜ì—ì„œ #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë¥¼ ê°„ë‹¨íˆ íŒŒì‹±í•©ë‹ˆë‹¤.
    src/cleaner.pyì—ì„œ ìº¡ì…˜ ì •ì œ ì‹œ ë” ì •êµí•˜ê²Œ í•´ì‹œíƒœê·¸ë¥¼ ì¶”ì¶œí•˜ì—¬
    post['hashtags_in_caption'] í•„ë“œì— ì €ì¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.)
    """
    all_hashtags = []
    for post in posts:
        caption = post.get("caption", "")
        found_hashtags = [word[1:] for word in caption.split() if word.startswith("#")]
        all_hashtags.extend(found_hashtags)
    
    hashtag_counts = Counter(all_hashtags)
    return hashtag_counts.most_common(top_n)


def main():
    st.set_page_config(page_title="ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸°", layout="wide")
    st.title("ğŸ“¸ ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸° (ê³µê°œ ë°ì´í„° ê¸°ë°˜)")

    if not UPSTAGE_API_KEY:
        st.error("âŒ UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜, API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()
    if not APIFY_API_TOKEN:
        st.error("âŒ APIFY_API_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜, API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()

    tab1, tab2, tab3 = st.tabs(["ğŸ” í•´ì‹œíƒœê·¸ ë¶„ì„", "ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰", "ğŸ§  ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ (ë©€í‹°í„´)"])

    with tab1:
        st.header("í•´ì‹œíƒœê·¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸")
        
        initial_hashtag_value = ""
        if "suggested_hashtag" in st.session_state:
            initial_hashtag_value = st.session_state.suggested_hashtag
            del st.session_state.suggested_hashtag

        hashtag = st.text_input(
            "ë¶„ì„í•  í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì—¬í–‰, ootd). ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ 'ì¼ìƒ'ìœ¼ë¡œ ìë™ ë¶„ì„ë©ë‹ˆë‹¤:",
            value=initial_hashtag_value,
            key="main_hashtag_input"
        ).strip().replace("#", "")
        
        # í•´ì‹œíƒœê·¸ ì…ë ¥ì´ ì—†ì„ ê²½ìš° 'ì¼ìƒ'ìœ¼ë¡œ ìë™ ì„¤ì •
        if not hashtag:
            hashtag_to_analyze = "ì¼ìƒ"
            st.info(f"í•´ì‹œíƒœê·¸ê°€ ì…ë ¥ë˜ì§€ ì•Šì•„ ê¸°ë³¸ í•´ì‹œíƒœê·¸ '#{hashtag_to_analyze}'ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        else:
            hashtag_to_analyze = hashtag

        max_posts = st.slider("ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜", min_value=20, max_value=500, value=50, step=1)

        if st.button("ë¶„ì„ ì‹œì‘"):
            if hashtag_to_analyze:
                posts = run_pipeline(hashtag_to_analyze, max_posts)
                
                if posts:
                    st.session_state["analyzed_posts"] = posts
                    
                    with st.spinner("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘..."):
                        st.session_state["vectorstore"] = build_vectorstore_from_posts(posts)
                    st.success("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")

                    st.subheader("ğŸ“ˆ ê°ì • ë¶„ì„ ê²°ê³¼")
                    plot_sentiment_distribution(posts)

                    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    generate_wordcloud(posts, font_path=FONT_PATH)

                    st.subheader("ğŸ”— ê´€ë ¨ í•´ì‹œíƒœê·¸ ì œì•ˆ")
                    top_hashtags = get_hashtag_frequency(posts, top_n=15)
                    if top_hashtags:
                        st.write("ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í•´ì‹œíƒœê·¸ë“¤ì…ë‹ˆë‹¤:")
                        cols = st.columns(5)
                        for i, (tag, count) in enumerate(top_hashtags):
                            with cols[i % 5]:
                                if st.button(f"#{tag} ({count})", key=f"suggested_tag_{tag}"):
                                    st.session_state.suggested_hashtag = tag
                                    st.experimental_rerun()
                    else:
                        st.info("ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ë‚´ì—ì„œ ë‹¤ë¥¸ í•´ì‹œíƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    st.subheader("ğŸ”¥ ìµœê·¼ ì¸ê¸° ê²Œì‹œë¬¼")
                    sorted_posts = sorted(posts, key=lambda x: x.get('likes', 0), reverse=True)
                    if sorted_posts:
                        st.write("ìµœê·¼ 1ì¼ ì´ë‚´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì¤‘ ì¢‹ì•„ìš”ê°€ ë§ì€ ê²Œì‹œë¬¼ì…ë‹ˆë‹¤:")
                        for i, post in enumerate(sorted_posts[:5]):
                            st.markdown(f"**{i+1}.** **ì¢‹ì•„ìš”: {post.get('likes', 0)}**")
                            st.markdown(f"   **ìº¡ì…˜:** {post.get('caption', '')[:150]}...")
                            st.markdown(f"   [ê²Œì‹œë¬¼ ë³´ê¸°]({post.get('url', '#')})")
                            st.markdown("---")
                    else:
                        st.info("ìµœê·¼ 1ì¼ ì´ë‚´ì˜ ì¸ê¸° ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                else:
                    st.warning("ë¶„ì„ì„ ìœ„í•œ ê²Œì‹œê¸€ì´ ì¶©ë¶„íˆ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            else:
                st.warning("í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


    with tab2:
        st.header("í‚¤ì›Œë“œ ê²€ìƒ‰")
        posts = st.session_state.get("analyzed_posts", [])

        if posts:
            st.info("ì¢Œì¸¡ 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            search_term = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥:")
            if search_term:
                results = search_by_keyword(posts, search_term)
                st.write(f"'{search_term}' í‚¤ì›Œë“œ í¬í•¨ ê²Œì‹œê¸€: {len(results)}ê°œ")
                for i, post in enumerate(results[:5], 1):
                    st.markdown(f"**{i}.** {post['cleaned_caption']}")
            elif search_term == "":
                st.info("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.info("ë¨¼ì € 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")

    with tab3:
        st.header("ì§ˆë¬¸ ê¸°ë°˜ ìš”ì•½ (ë©€í‹°í„´)")
        posts = st.session_state.get("analyzed_posts", [])
        vectorstore = st.session_state.get("vectorstore", None)

        if posts and vectorstore:
            st.info("ì¢Œì¸¡ 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
            
            if "chat_history" not in st.session_state:
                st.session_state.chat_history = []

            question = st.text_input("ğŸ—£ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="multi_q")

            if st.button("ğŸ’¬ ì§ˆë¬¸ ë³´ë‚´ê¸°"):
                if question:
                    with st.spinner("Solar ì‘ë‹µ ìƒì„± ì¤‘..."):
                        answer = solar_rag_answer_multi(
                            question=question,
                            history=st.session_state.chat_history,
                            vectorstore=vectorstore,
                            api_key=UPSTAGE_API_KEY
                        )
                        st.session_state.chat_history.append((question, answer))
                else:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            if st.button("ğŸ§¹ ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”"):
                st.session_state.chat_history = []
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

            st.markdown("---")
            st.markdown("## ğŸ’¬ ëŒ€í™” ì´ë ¥")
            if st.session_state.chat_history:
                for idx, (q, a) in enumerate(st.session_state.chat_history[::-1], 1):
                    with st.expander(f"Q{len(st.session_state.chat_history) - idx + 1}: {q}"):
                        st.markdown(f"**AI:** {a}")
            else:
                st.info("ì•„ì§ ëŒ€í™” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì‹œì‘í•´ ë³´ì„¸ìš”!")
        else:
            st.info("ë¨¼ì € 'í•´ì‹œíƒœê·¸ ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    # --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ì‹œì‘ ---
    # ì´ ë¶€ë¶„ì€ Streamlit ì•±ì´ src ë° visualization ëª¨ë“ˆì„ ì˜¬ë°”ë¥´ê²Œ ì°¾ë„ë¡ í•©ë‹ˆë‹¤.
    current_script_path = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_script_path)
    project_root_dir = os.path.join(current_dir, "..")
    sys.path.insert(0, project_root_dir)
    # --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ë ---
    main()