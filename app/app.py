# app/app.py

import sys
import os
# --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ì‹œì‘ ---
current_script_path = os.path.abspath(__file__)
current_dir = os.path.dirname(current_script_path)
project_root_dir = os.path.join(current_dir, "..")
sys.path.insert(0, project_root_dir)
# --- ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œ ì„¤ì • ë ---

import json 
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict

import streamlit as st
from dotenv import load_dotenv

# src ë° visualization ëª¨ë“ˆ ì„í¬íŠ¸
from src.cleaner import clean_captions
from src.sentiment import load_sentiment_model, analyze_sentiment
from src.keywords import load_keyword_model, extract_keywords, get_keyword_frequency
from src.search import search_by_keyword
from visualization.charts import plot_sentiment_distribution
from visualization.wordcloud import generate_wordcloud

# Langchain ë° Upstage ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate

# Apify í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from apify_client import ApifyClient

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")

# ì „ì—­ ì„¤ì •
DATA_DIR = "../data"
INSTAGRAM_SCRAPER_ACTOR_ID = "apify/instagram-hashtag-scraper"

# í°íŠ¸ ê²½ë¡œ ì„¤ì •
current_script_path = os.path.abspath(__file__)
current_dir = os.path.dirname(current_script_path)
project_root_dir = os.path.join(current_dir, "..")
FONT_PATH = os.path.join(project_root_dir, "fonts", "NotoSansKR-Regular.ttf")


def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    defaults = {
        "analyzed_posts": [],
        "vectorstore": None,
        "chat_history": [],
        "current_hashtag": "",
        "current_max_posts": 50
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def fetch_posts_from_apify(hashtag: str, max_count: int, apify_api_token: str) -> List[Dict]:
    """Apifyë¥¼ í†µí•œ ì¸ìŠ¤íƒ€ê·¸ë¨ ë¦´ìŠ¤ ìˆ˜ì§‘"""
    if not apify_api_token:
        st.error("âŒ Apify API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    client = ApifyClient(apify_api_token)
    run_input = {
        "hashtags": [hashtag],
        "resultsLimit": max_count,
        "proxyConfiguration": {"use": "AUTO_POOL"}
    }

    st.info(f"ğŸš€ í•´ì‹œíƒœê·¸ #{hashtag} ë¦´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ìµœëŒ€ {max_count}ê°œ)")
    
    try:
        run = client.actor(INSTAGRAM_SCRAPER_ACTOR_ID).call(
            run_input=run_input,
            timeout_secs=300
        )

        posts = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            caption = item.get("caption", "")
            if caption:
                date_str = item.get("timestamp", "")
                date_only = date_str.split("T")[0] if "T" in date_str else date_str.split(" ")[0]
                
                posts.append({
                    "caption": caption,
                    "date": date_only,
                    "shortcode": item.get("shortcode"),
                    "url": item.get("url"),
                    "hashtag": hashtag,
                    "likes": item.get("likesCount", 0),
                    "comments": item.get("commentsCount", 0),
                    "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
        
        st.success(f"âœ… {len(posts)}ê°œ ë¦´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts

    except Exception as e:
        st.error(f"âŒ ë¦´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def save_posts_to_json(posts: List[Dict], filename: str):
    """JSON íŒŒì¼ ì €ì¥"""
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        filepath = os.path.join(DATA_DIR, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def run_analysis_pipeline(hashtag: str, max_posts: int) -> List[Dict]:
    """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    with st.spinner("ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
        # 1. ë°ì´í„° ìˆ˜ì§‘
        posts = fetch_posts_from_apify(hashtag, max_posts, APIFY_API_TOKEN)
        if not posts:
            return []

        # 2. ë°ì´í„° ì €ì¥
        save_posts_to_json(posts, f"instagram_{hashtag}_raw.json")

        # 3. í…ìŠ¤íŠ¸ ì •ì œ
        st.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
        posts = clean_captions(posts)

        # 4. ê°ì • ë¶„ì„
        st.info("ğŸ“Š ê°ì • ë¶„ì„ ì¤‘...")
        sentiment_model = load_sentiment_model()
        posts = analyze_sentiment(posts, sentiment_model)

        # 5. í‚¤ì›Œë“œ ì¶”ì¶œ
        st.info("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keyword_model = load_keyword_model()
        posts = extract_keywords(posts, keyword_model)

        # 6. ìµœì¢… ê²°ê³¼ ì €ì¥
        save_posts_to_json(posts, f"instagram_{hashtag}_final.json")

        st.success("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        return posts


def build_vectorstore(posts: List[Dict]) -> FAISS:
    """ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
    docs = [
        Document(page_content=p["cleaned_caption"]) 
        for p in posts 
        if p.get("cleaned_caption") and len(p["cleaned_caption"].strip()) > 0
    ]

    if not docs:
        st.warning("âš ï¸ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•ì— í•„ìš”í•œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    embeddings = UpstageEmbeddings(model="embedding-passage")
    return FAISS.from_documents(docs, embeddings)


def get_rag_answer(question: str, history: list, vectorstore: FAISS) -> str:
    """RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€"""
    if not UPSTAGE_API_KEY:
        return "ì˜¤ë¥˜: Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if not vectorstore:
        return "ë¶„ì„ëœ ë¦´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•´ì‹œíƒœê·¸ ë¶„ì„ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."

    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    context_docs = retriever.invoke(question)
    context = "\n".join([doc.page_content for doc in context_docs])

    # ëŒ€í™” ê¸°ë¡ êµ¬ì„±
    chat_history = "\n".join([f"ì‚¬ìš©ì: {q}\nAI: {a}" for q, a in history])

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒì€ ì¸ìŠ¤íƒ€ê·¸ë¨ ë¦´ìŠ¤ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì™€ ë¦´ìŠ¤ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{chat_history}

[ë¦´ìŠ¤ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
""")
    
    # LLM í˜¸ì¶œ
    llm = ChatUpstage(model="solar-1-mini-chat", api_key=UPSTAGE_API_KEY)
    chain = prompt | llm
    
    response = chain.invoke({
        "question": question,
        "context": context,
        "chat_history": chat_history
    })
    return response.content


def get_hashtag_frequency(posts: List[Dict], top_n: int = 10) -> List[tuple]:
    """í•´ì‹œíƒœê·¸ ë¹ˆë„ ê³„ì‚°"""
    all_hashtags = []
    for post in posts:
        caption = post.get("caption", "")
        hashtags = [word[1:] for word in caption.split() if word.startswith("#")]
        all_hashtags.extend(hashtags)
    
    return Counter(all_hashtags).most_common(top_n)


def render_analysis_results(posts: List[Dict]):
    """ë¶„ì„ ê²°ê³¼ ë Œë”ë§"""
    if not posts:
        return

    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“ˆ ê°ì • ë¶„ì„")
        plot_sentiment_distribution(posts, font_path=FONT_PATH)
        
        st.subheader("ğŸ”— ê´€ë ¨ í•´ì‹œíƒœê·¸")
        top_hashtags = get_hashtag_frequency(posts, top_n=10)
        if top_hashtags:
            for tag, count in top_hashtags:
                if st.button(f"#{tag} ({count})", key=f"tag_{tag}"):
                    st.session_state.current_hashtag = tag
                    st.rerun()
    
    with col2:
        st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        if os.path.exists(FONT_PATH):
            try:
                generate_wordcloud(posts, font_path=FONT_PATH)
            except Exception as e:
                st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        else:
            st.error("í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        st.subheader("ğŸ”¥ ì¸ê¸° ë¦´ìŠ¤")
        sorted_posts = sorted(posts, key=lambda x: x.get('likes', 0), reverse=True)
        for i, post in enumerate(sorted_posts[:3], 1):
            st.markdown(f"**{i}.** ğŸ‘ {post.get('likes', 0)}")
            st.markdown(f"{post.get('caption', '')[:100]}...")
            if post.get('url'):
                st.markdown(f"[ë³´ê¸°]({post['url']})")
            st.markdown("---")


def render_search_section(posts: List[Dict]):
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ì„¹ì…˜"""
    st.subheader("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
    
    if not posts:
        st.info("ë¨¼ì € í•´ì‹œíƒœê·¸ ë¶„ì„ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
        return
    
    search_term = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œ:", key="search_keyword")
    if search_term:
        results = search_by_keyword(posts, search_term)
        st.write(f"**'{search_term}'** í¬í•¨ ë¦´ìŠ¤: **{len(results)}ê°œ**")
        
        for i, post in enumerate(results[:5], 1):
            st.markdown(f"**{i}.** {post.get('cleaned_caption', '')[:200]}...")


def render_qa_section(posts: List[Dict], vectorstore: FAISS):
    """ì§ˆë¬¸ë‹µë³€ ì„¹ì…˜"""
    st.subheader("ğŸ§  ì§ˆë¬¸ ê¸°ë°˜ ë¶„ì„")
    
    if not posts or not vectorstore:
        st.info("ë¨¼ì € í•´ì‹œíƒœê·¸ ë¶„ì„ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
        return
    
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="question_input")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°"):
            if question:
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    answer = get_rag_answer(question, st.session_state.chat_history, vectorstore)
                    st.session_state.chat_history.append((question, answer))
    
    with col2:
        if st.button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    if st.session_state.chat_history:
        st.markdown("### ğŸ’¬ ëŒ€í™” ê¸°ë¡")
        for i, (q, a) in enumerate(reversed(st.session_state.chat_history), 1):
            with st.expander(f"Q{len(st.session_state.chat_history) - i + 1}: {q}"):
                st.markdown(f"**ë‹µë³€:** {a}")


def main():
    st.set_page_config(page_title="ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸°", layout="wide")
    st.title("ğŸ“± ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ë¶„ì„ê¸°")

    # API í‚¤ í™•ì¸
    if not UPSTAGE_API_KEY or not APIFY_API_TOKEN:
        st.error("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()

    # ë©”ì¸ ì…ë ¥ ì„¹ì…˜
    st.header("ğŸ” í•´ì‹œíƒœê·¸ ë¶„ì„")
    
    col1, col2, col3 = st.columns([3, 2, 1])
    
    with col1:
        hashtag = st.text_input(
            "ë¶„ì„í•  í•´ì‹œíƒœê·¸ ì…ë ¥!!!",
            value=st.session_state.current_hashtag,
            placeholder="ì˜ˆ: ì—¬í–‰, ootd, ì‚¬ë‘ (ê¸°ë³¸ê°’: ì¼ìƒ)"
        ).strip().replace("#", "")
        
    with col2:
        max_posts = st.slider(
            "ìˆ˜ì§‘í•  ë¦´ìŠ¤ ìˆ˜:", 
            min_value=20, 
            max_value=500, 
            value=st.session_state.current_max_posts
        )
        
    with col3:
        st.markdown("<br>", unsafe_allow_html=True)  # ë²„íŠ¼ ìœ„ì¹˜ ì¡°ì •
        analyze_button = st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary")

    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state.current_hashtag = hashtag
    st.session_state.current_max_posts = max_posts

    # ë¶„ì„ ì‹¤í–‰
    if analyze_button:
        hashtag_to_analyze = hashtag if hashtag else "ì¼ìƒ"
        posts = run_analysis_pipeline(hashtag_to_analyze, max_posts)
        
        if posts:
            st.session_state.analyzed_posts = posts
            with st.spinner("ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘..."):
                vectorstore = build_vectorstore(posts)
                st.session_state.vectorstore = vectorstore

    # ê²°ê³¼ í‘œì‹œ - ì¢Œì¸¡ 75% + ìš°ì¸¡ 25% ë ˆì´ì•„ì›ƒ
    posts = st.session_state.analyzed_posts
    vectorstore = st.session_state.vectorstore

    if posts:
        st.markdown("---")
        
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ: ì¢Œì¸¡ 75% (ë¶„ì„ ê²°ê³¼) + ìš°ì¸¡ 25% (ê²€ìƒ‰/ì§ˆë¬¸ë‹µë³€)
        main_col, sidebar_col = st.columns([3, 1])
        
        with main_col:
            # ë¶„ì„ ê²°ê³¼ (2ì—´ ë ˆì´ì•„ì›ƒ)
            render_analysis_results(posts)
            
        with sidebar_col:
            # ìš°ì¸¡ ì‚¬ì´ë“œë°”: ê²€ìƒ‰ & ì§ˆë¬¸ë‹µë³€ (ì„¸ë¡œ ë°°ì¹˜)
            render_search_section(posts)
            st.markdown("---")
            render_qa_section(posts, vectorstore)
            
    else:
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ ìœ ì§€ (ë¶„ì„ ì „ì—ë„ ìš°ì¸¡ ì˜ì—­ í‘œì‹œ)
        main_col, sidebar_col = st.columns([3, 1])
        
        with main_col:
            st.info("í•´ì‹œíƒœê·¸ë¥¼ ì…ë ¥í•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
        with sidebar_col:
            st.info("ë¶„ì„ í›„ ê²€ìƒ‰ê³¼ ì§ˆë¬¸ë‹µë³€ ê¸°ëŠ¥ì„ ì´ìš©í•˜ì„¸ìš”.")


if __name__ == "__main__":
    # ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
    current_script_path = os.path.abspath(__file__)
    current_dir = os.path.dirname(current_script_path)
    project_root_dir = os.path.join(current_dir, "..")
    sys.path.insert(0, project_root_dir)
    
    main()