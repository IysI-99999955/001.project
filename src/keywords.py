# src/keywords.py
import json
import os
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
from collections import Counter

def load_keyword_model(model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS") -> Optional[KeyBERT]:
    """
    KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë”©
    :param model_name: SBERT ì„ë² ë”© ëª¨ë¸ ì´ë¦„
    :return: KeyBERT ëª¨ë¸ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        print(f"í•œêµ­ì–´ SBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print("âš ï¸  ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        sbert_model = SentenceTransformer(model_name)
        kw_model = KeyBERT(sbert_model)
        
        print("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return kw_model
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install keybert sentence-transformers")
        return None

def extract_keywords(posts: List[Dict], model: KeyBERT, top_n: int = 3) -> List[Dict]:
    """
    ê° ê²Œì‹œê¸€ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
    :param posts: ì •ì œ ë° ê°ì • ë¶„ì„ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    :param model: KeyBERT ëª¨ë¸
    :param top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
    :return: keywords í•„ë“œê°€ ì¶”ê°€ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    if not model:
        print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return posts
    
    print(f"ğŸ” {len(posts)}ê°œ ê²Œì‹œê¸€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
    
    for i, post in enumerate(posts):
        try:
            # cleaned_caption í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            text = post.get("cleaned_caption", post.get("caption", ""))
            
            if not text or len(text.strip()) < 10:
                post["keywords"] = []
                continue
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = model.extract_keywords(
                text, 
                top_n=top_n,
                keyphrase_ngram_range=(1, 2),  # 1-2ë‹¨ì–´ í‚¤ì›Œë“œ
                stop_words=None,
                use_maxsum=True,  # ë‹¤ì–‘ì„± ì¦ê°€
                nr_candidates=20,  # í›„ë³´ í‚¤ì›Œë“œ ìˆ˜
                diversity=0.5  # ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„°
            )
            
            # í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ì ìˆ˜ ì œì™¸)
            post["keywords"] = [kw[0] for kw in keywords]
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            if (i + 1) % 10 == 0:
                print(f"  {i + 1}/{len(posts)} ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            print(f"  ê²Œì‹œê¸€ {i+1} í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            post["keywords"] = []
    
    print("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
    return posts

def get_keyword_frequency(posts: List[Dict], min_length: int = 2) -> Counter:
    """
    ì „ì²´ ê²Œì‹œê¸€ì—ì„œ ë“±ì¥í•œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    :param posts: keywords í•„ë“œ í¬í•¨ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    :param min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´
    :return: í‚¤ì›Œë“œ ë¹ˆë„ Counter ê°ì²´
    """
    all_keywords = []
    
    for post in posts:
        keywords = post.get("keywords", [])
        # ìµœì†Œ ê¸¸ì´ í•„í„°ë§
        filtered_keywords = [kw for kw in keywords if len(kw) >= min_length]
        all_keywords.extend(filtered_keywords)
    
    print(f"ğŸ“Š ì´ {len(all_keywords)}ê°œì˜ í‚¤ì›Œë“œ ë°œê²¬")
    return Counter(all_keywords)

def save_keyword_analysis(posts: List[Dict], keyword_freq: Counter, save_path: str) -> None:
    """
    í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
    :param posts: í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    :param keyword_freq: í‚¤ì›Œë“œ ë¹ˆë„ Counter
    :param save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    try:
        # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        analysis_result = {
            "posts": posts,
            "keyword_analysis": {
                "total_keywords": len(keyword_freq),
                "top_keywords": keyword_freq.most_common(20),
                "keyword_frequency": dict(keyword_freq)
            },
            "analysis_date": json.dumps({"timestamp": "2024-01-01 00:00:00"})  # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥: {save_path}")
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_analyzed_posts(file_path: str) -> List[Dict]:
    """
    ë¶„ì„ëœ ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ
    :param file_path: íŒŒì¼ ê²½ë¡œ
    :return: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        if not os.path.exists(file_path):
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return []
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡° í™•ì¸
        if isinstance(data, list):
            posts = data
        elif isinstance(data, dict) and "posts" in data:
            posts = data["posts"]
        else:
            print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤.")
            return []
        
        print(f"âœ… {len(posts)}ê°œì˜ ê²Œì‹œê¸€ ë¡œë“œ ì™„ë£Œ")
        return posts
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ë¶„ì„ ì‹œì‘")
    
    # ëª¨ë¸ ë¡œë”©
    kw_model = load_keyword_model()
    if not kw_model:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    input_file = "../data/ì—¬í–‰_posts_analyzed.json"
    
    # ë¶„ì„ëœ ê²Œì‹œê¸€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    posts = load_analyzed_posts(input_file)
    if not posts:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    posts_with_keywords = extract_keywords(posts, kw_model, top_n=5)
    
    # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
    keyword_freq = get_keyword_frequency(posts_with_keywords)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ìƒìœ„ í‚¤ì›Œë“œ TOP 10:")
    for keyword, count in keyword_freq.most_common(10):
        print(f"  {keyword}: {count}íšŒ")
    
    # ê²°ê³¼ ì €ì¥
    output_file = "../data/ì—¬í–‰_posts_keywords.json"
    save_keyword_analysis(posts_with_keywords, keyword_freq, output_file)
    
    print("\nâœ… í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()